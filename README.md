## status projektu
- âœ… Dokumentacja
- âœ… Link scraper 
- âœ… Content scraper
- âœ… pickle â¡ csv 
- ğŸš« Content processor
- ğŸš« Dokumentacja (eng)

## TLDR - Uproszczona instrukcja

Kod sÅ‚uÅ¼y do pobrania artykuÅ‚Ã³w z witryny `tvp.info.pl`. W celu wykonania kodu naleÅ¼y z poziomu terminala wywoÅ‚aÄ‡ poniÅ¼sze komendy.

### przygotowanie Å›rodowiska:

```bash
conda create -n scraper-env python=3.11
conda activate scraper-env
pip install -r requirements.txt
```

### pobranie metadanych:

```bash
python scraper_tvp_links.py --domain=polska --start_page=1 --end_page=10
```

### pobranie zawartoÅ›ci artykuÅ‚Ã³w:

```bash
python scraper_tvp_content.py --n_workers=2 --n_batches=2 --batch_size=16
```
PobranÄ… zawartoÅ›Ä‡ moÅ¼na znaleÅºÄ‡ w folderze:
```bash
project/obtained_content
```
___
___

## Cel projektu

Celem projektu byÅ‚o pobranie duÅ¼ej iloÅ›ci danych tekstowych w celu dostarczenia danych treningowych dla modelu jÄ™zykowego, ktÃ³rego celem miaÅ‚o byÄ‡ generowanie streszczenia tekstu.

WybÃ³r padÅ‚ na witrynÄ™ `tvp.info.pl` z doÅ›Ä‡ prostego powodu - oferuje dostÄ™p do danych archiwalnych z lat 2006 - 2023 w sposÃ³b nieodpÅ‚atny. Dane postanowiono zgromadziÄ‡ wykorzystujÄ…c do tego techniki web scrapingu. Projekt obejmuje proces przygotowania kodu potrzebnego do pozyskania danych w sposÃ³b zautomatyzowany i efektywny.  

Ostatecznie udaÅ‚o siÄ™ pozyskaÄ‡ ponad 250 tysiÄ™cy arykuÅ‚Ã³w z rÃ³Å¼nych domen: sport, biznes, polska, Å›wiat, spoÅ‚eczeÅ„stwo i wiele innych.Â 

---
## Struktura repo

```
ğŸ“¦ scraper-tvp
â”£ğŸ“‚ articles metadata poÅ‚Ä…czone metadane
â”ƒ â”— ğŸ“œ joined_metadata_files.csv
â”£ğŸ“‚obtained content #logi wykonania scrpaera i finalny plik
â”ƒ â”£ ğŸ“œ full_results.csv
â”ƒ â”— ğŸ“œ logs.json
â”£ğŸ“‚ results #pobrane metadane w osobnych plikach
â”ƒ â”— ğŸ“œ results_domain_start-page-last-page.csv
â”£ğŸ“‚ src #kod
â”ƒ â”£ ğŸ“œ scraper_tvp_content.py
â”ƒ â”£ ğŸ“œ scraper_tvp_links.py 
â”ƒ â”— ğŸ“œ text_processing.py (TODO)
```
Kod zostaÅ‚ podzielony na moduÅ‚y tworzÄ…ce (przynajmniej w teorii) logiczny ukÅ‚ad.  
Role poszczegÃ³lnych moduÅ‚Ã³w sÄ… nastÄ™pujÄ…ce:
- [utils](#funkcje-pomocnicze) - zawiera funkcje pomocnicze
- [links scraper](#links-scraper) - to plik wykonywalny sÅ‚uÅ¼Ä…cy do pobrania linkÃ³w do artyuÅ‚Ã³w z okreÅ›lonej iloÅ›ci stron z danej domeny.
- [links scraper](#content-scraper) - plik wykonywalny sÅ‚uÅ¼Ä…cy do pobierania treÅ›ci artykuÅ‚Ã³w

ğŸ“‚ `results` zawiera metadane pobrane przez `scraper_links_tvp.py` w postaci pliku `.csv`.

ğŸ“‚ `articles_metadata` zawiera plik, ktÃ³ry jest wynikiem poÅ‚Ä…czenia plikÃ³w z folderu `results`. Przechowywany jest w nim plik, z ktÃ³rego `scraper_tvp_content.py` wczytuje metadane dotyczÄ…ce artykuÅ‚Ã³w i pobiera ich zawartoÅ›Ä‡.

ğŸ“‚ `obtained_content` zawiera plik `.csv`, ktÃ³ry zawiera dotychczasowo pobrane treÅ›ci artykuÅ‚Ã³w wraz z ich metadanymi. Z tego folderu pobierane sÄ… ostateczne dane. W kaÅ¼dym wywoÅ‚aniu kodu `scraper_tvp_content.py` plik jest wczytywany, a nastÄ™pnie kaÅ¼dy `batch` jest dopisywany do pliku. Dodatkowo w tym folderze umieszczono plik `logs.json`, ktÃ³ry zawiera informacje o postÄ™pie pobierania danych. KaÅ¼de kolejne wywoÅ‚anie funkcji zacznie pobieranie zawartoÅ›ci artykuÅ‚Ã³w w miejscu, ktÃ³re zapisane zostaÅ‚o w pliku logÃ³w.



---
### content scraper

Zadaniem programu `scraper_tvp_content.py` jest pobranie treÅ›ci artykuÅ‚Ã³w z danych linkÃ³w.

Program przygotowuje plik z metadanymi artykuÅ‚Ã³w - scala wiele plikÃ³w w jeden a nastÄ™pnie je wczytuje i pobiera zawartoÅ›Ä‡ linkÃ³w. Dane pobierane sÄ… rÃ³wnolegle przy wykorzystaniu zadanej przez uÅ¼ytkownika iloÅ›ci procesÃ³w.

W celu unikniÄ™cia utraty danych na wskutek potencjalnej awarii lub niepowodzenia w pobraniu danych program pobiera dane w seriach o okreÅ›lonej z gÃ³ry wielkoÅ›ci. KaÅ¼de kolejne wywoÅ‚anie funkcji nie wpÅ‚ywa na dotychczasowo pobrane dane, gdyz funkcja jedynie dopisuje nowe rekordy do pliku.

Po pobraniu kaÅ¼dej serii program odczekuje losowÄ… iloÅ›Ä‡ czasu. PrawdopodobieÅ„stwo odczekiwania przez dÅ‚uÅ¼szy czas jest mniejsze niÅ¼ odczekiwanie przez krÃ³tki okres czasu.

Pobrane dane zapisywane sÄ… do pliku z roszerzeniem `.csv`. Przy kolejnym wywoÅ‚aniu funkcja z pliku `logs.json` wczytuje stan ostatniego wykonania i zaczyna pobieraÄ‡ dane od tego miejsca.Â 

---
### links scraper

Program `scrper_tvp_links.py` pobiera linki do artykuÅ‚Ã³w z danej domeny ze stron o numerach zadeklarowanych przez uÅ¼ytkownika. Linki wraz z tytuÅ‚em oraz leadem artykuÅ‚u zapisywane sÄ… do pliku z roszerzeniem `.csv`. Pobrane dane wczytywane sÄ… nastÄ™pnie przez moduÅ‚ `scraper_tvp_content.py` i przez niego pobierane sÄ… zawartoÅ›ci artykuÅ‚Ã³w.

---
### utils 

ModuÅ‚ pomocniczy zawiera rÃ³Å¼ne funkcje zwiÄ…zane z wykonywaniem kodu.

---

## PrzepÅ‚yw danych

PoniÅ¼ej przedstawiono schematyczny przepÅ‚yw danych. `link_scraper.py` wysyÅ‚a zapytanie do strony i zwraca linki do artykuÅ‚Ã³w, ktÃ³re nastÄ™pnie sÄ… zapisywane do pliku `.csv`. KaÅ¼de wywoÅ‚anie programu tworzy nowy plik. Wszystkie pliki sÄ… nastÄ™pnie Å‚Ä…czone w jeden, ktÃ³ry wczytywany jest przez `content_scraper.py`. Scraper ponownie Å‚Ä…czy siÄ™ ze stronÄ… (tym razem za poÅ›rednictwem linku do artykuÅ‚Ã³w) i w rezultacie zwraca plik z pobranymi danymi. Dodatkowo generowany jest plik z logami, ktÃ³ry weryfikowany jest przy kaÅ¼dym kolejnym wywoÅ‚aniu funkcji.

![przepÅ‚yw danych](dataflow.png)


## PrzykÅ‚adowe uÅ¼ycie

### Przygotowanie Å›rodowiska

W celu przygotowania Å›rodowiska naleÅ¼y wykonaÄ‡ poniÅ¼sze komendy w terminalu:

```bash
conda create -n scraper-env python=3.11
conda activate scraper-env
pip install -r requirements.txt
```

### pobieranie linkÃ³w do artykuÅ‚Ã³w

ModuÅ‚ `scrper_tvp_links.py` wywoÅ‚ywany jest z kilkoma parametrami. SÄ… one ustawiane w momencie uruchamiania programu w terminalu.

* `domain` - sekcja, z ktÃ³rej majÄ… zostaÄ‡ pobrane linki. DostÄ™pne (i przetestowane) opcje to biznes, polska, swiat, spoleczenstwo, sport oraz kultura.
* `start_page` - numer strony, od ktÃ³rego program ma zaczÄ…Ä‡ pobieraÄ‡ linki
* `end_page` - numer ostatniej strony do pobrania

Pobrane metadane przechowywane sÄ… w pliku  `results_<domena>_<strona-startowa>-<strona-koncowa>.csv` i zawierajÄ… informacje o linku do artykuÅ‚u, tytule oraz leadzie.

Wykonanie programu odbywa siÄ™ w terminalu poprzez wywoÅ‚anie z poziomu folderu `src` komend:

```bash
python scraper_tvp_links.py --domain=polska --start_page=1 --end_page=4
```

![przykÅ‚adowe wykonanie kodu](demo_links.gif)

---
### pobieranie zawartoÅ›ci linkÃ³w (peÅ‚nych artykuÅ‚Ã³w)

Program `scraper_tvp_content.py` rÃ³wnieÅ¼ uruchamiany jest z kilkoma parametrami:

* `n_workers` odpowiada za iloÅ›Ä‡ procesÃ³w, przy uÅ¼yciu ktÃ³rych wykonywany bÄ™dzie kod. DomyÅ›lnie ustawiona jest ona na iloÅ›Ä‡ dostÄ™pnych rdzeni.Â 
* `batch_size` odpowiada za iloÅ›Ä‡ pobieranych linkÃ³w w jednym wykonaniu. KaÅ¼dy `batch` tworzy zadanÄ… iloÅ›Ä‡ procesÃ³w, ktÃ³re istniejÄ… do koÅ„ca pobierania danej serii.
* `n_batches` odpowiada za iloÅ›Ä‡ serii do pobrania.

PrzykÅ‚adowo, przy `batch_size = 64`, `n_batches = 4` i `n_workers = 4` w jednym wywoÅ‚aniu programu zostanie pobrana zawartoÅ›Ä‡ 256 artykuÅ‚Ã³w przy wykorzystaniu 4 procesÃ³w. Â 

Opis poszczegÃ³lnych parametrÃ³w moÅ¼na wyÅ›wietliÄ‡ nastÄ™pujÄ…co:

```bash
python scraper_tvp_content.py --help
```

W celu wykonania programu naleÅ¼y z poziomu folderu `src` wykonaÄ‡ komendÄ™:

```bash
python scraper_tvp_content.py --n_workers=2 --n_batches=2 --batch_size=16
```

![[demo_content.gif]]

---
## DostÄ™p do danych

Pozyskane dane zostaÅ‚y opublikowane na platformie `Hugging Face`. MoÅ¼na je pobraÄ‡ [stÄ…d](https://huggingface.co/datasets/WiktorS/polish-news), bÄ…dÅº wczytaÄ‡ bezpoÅ›rednio z poziomu kodu wykorzystujÄ…c do tego API platformy :

```bash
pip install datasets
```

```python
from datasets import load_dataset

dataset = load_dataset("WiktorS/polish-news")
```
